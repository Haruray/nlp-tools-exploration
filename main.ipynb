{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas 1 : Basic NLP Tools\n",
    "Oleh : \n",
    "- Safiq Faray (13519145)\n",
    "\n",
    "Eksplorasi Basic NLP Tools dengan `SpaCy`. \n",
    "\n",
    "Library `SpaCy` di import terlebih dahulu, lalu membuat sample text singkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = nlp(\"Hello bro, i live in Indonesia and i love lasagna. Also, i love burgers and pizzas. Man, i wish i could get some pizza right now. I worked at itb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Splitter\n",
    "\n",
    "Pada tools pertama, dibuat fungsi `sentence_splitter` yang memanfaatkan attribute `.sents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hello bro, i live in Indonesia and i love lasagna.,\n",
       " Also, i love burgers and pizzas.,\n",
       " Man, i wish i could get some pizza right now.,\n",
       " I worked at itb]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_split(text):\n",
    "    return list(text.sents)\n",
    "sentence_split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Pada doc (text) di `SpaCy`, terdiri dari beberapa token. Pada fungsi `tokenization`, token-token tersebut dikumpulkan di sebuah list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'bro',\n",
       " ',',\n",
       " 'i',\n",
       " 'live',\n",
       " 'in',\n",
       " 'Indonesia',\n",
       " 'and',\n",
       " 'i',\n",
       " 'love',\n",
       " 'lasagna',\n",
       " '.',\n",
       " 'Also',\n",
       " ',',\n",
       " 'i',\n",
       " 'love',\n",
       " 'burgers',\n",
       " 'and',\n",
       " 'pizzas',\n",
       " '.',\n",
       " 'Man',\n",
       " ',',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'i',\n",
       " 'could',\n",
       " 'get',\n",
       " 'some',\n",
       " 'pizza',\n",
       " 'right',\n",
       " 'now',\n",
       " '.',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'at',\n",
       " 'itb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    return [token.text for token in text]\n",
    "tokenization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Pada `SpaCy`, tidak ada method atau attribute stemming, sehingga dibuat fungsi custom yang akan me-cut prefix dan suffix dari sebuah word. Pembuatan fungsi ini sangat sederhana, sehingga ada kemungkinan bahwa jika digunakan pada kata lain, tidak akan menghasilkan stemming yang tepat.\n",
    "\n",
    "Untuk hasil yang lebih tepat, dapat digunakan `PorterStemming` pada library `nltk` (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(token):\n",
    "    prefix = token.prefix_\n",
    "    suffix = token.suffix_\n",
    "    original = token.text\n",
    "    #stem prefix\n",
    "    stemmed = original[len(prefix)-1:len(original)]\n",
    "    #stem suffix\n",
    "    stemmed = stemmed[0:(len(stemmed)-1) - (len(suffix)-1)]\n",
    "    return stemmed\n",
    "tes = nlp(\"writing\")\n",
    "stemming(tes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization adalah bentuk dasar sebuah kata, yang dapat diakses pada attribute yang sudah disediakan oleh `SpaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatization(token):\n",
    "    return token.lemma_\n",
    "lemmatization(tes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Masking\n",
    "\n",
    "Pada `SpaCy`, sebuah kata dapat diakses properti entitasnya dengan `.ent_type_`. Jika kata tersebut bukanlah entitas seperti tempat, mata uang, dll, maka `token.ent_type` akan bernilai string kosong. Jika tidak, maka token tersebut diganti dengan `MASK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello bro , i live in MASK and i love lasagna . Also , i love burgers and pizzas . Man , i wish i could get some pizza right now . I worked at MASK'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_entity(text):\n",
    "    final_text = ' '.join(token.text if token.ent_type_ == \"\" else \"MASK\" for token in text)\n",
    "    return final_text\n",
    "mask_entity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Masking Custom Word\n",
    "\n",
    "Jika ada kata yang tidak terdeteksi sebagai entity, maka fungsi dibawah akan menandai custom words sebagai entitas sehingga akan ikut di masking. Jenis-jenis entity pada `SpaCy` telah di daftarkan sebagai Enum, jika ada entity baru yang ingin didaftarkan, maka `SpaCy` dapat di train dengan dataset tertentu.\n",
    "\n",
    "Sumber : https://www.analyticsvidhya.com/blog/2022/06/custom-named-entity-recognition-using-spacy-v3/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello MASK , i live in MASK and i love lasagna . Also , i love burgers and pizzas . Man , i wish i could get some pizza right now . I worked at MASK'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from spacy.tokens import Span\n",
    "EntityType = Enum('EntityType',['Person', 'Norp', 'Fac', 'Org', 'Gpe', 'Loc', 'Product', 'Event', 'Work_of_art', 'Law', 'Language','Date', 'Time', 'Percent', 'Money', 'Quantity', 'Ordinal', 'Cardinal'])\n",
    "\n",
    "def get_token_index(text, word):\n",
    "    idx_tokens = []\n",
    "    idx_begin = 0\n",
    "    for token in text:\n",
    "        if token.text == word:\n",
    "            idx_end = idx_begin+1\n",
    "            idx_tokens.append([idx_begin, idx_end])\n",
    "        idx_begin+=1\n",
    "    return idx_tokens\n",
    "\n",
    "def filter_unique_entities(new_ents, original_ents):\n",
    "    unique_ents = []\n",
    "    for ent in new_ents:\n",
    "        if (ent not in original_ents):\n",
    "            unique_ents.append(ent)\n",
    "    return unique_ents\n",
    "\n",
    "def create_new_entity(text, entity_word, entity_type: EntityType):\n",
    "    idx_tokens = get_token_index(text, entity_word)\n",
    "    if (len(idx_tokens) == 0):\n",
    "        raise Exception('Custom word to be defined as entity not found in text')\n",
    "    new_entities = []\n",
    "    org_ents = list(text.ents)\n",
    "    for it in idx_tokens:\n",
    "        new_entity = Span(text, it[0], it[1], label=entity_type.name.upper())\n",
    "        new_entities.append(new_entity)\n",
    "    new_entities = filter_unique_entities(new_entities, org_ents)\n",
    "    text.ents = org_ents + new_entities\n",
    "\n",
    "def mask_entity_custom(text, entity_word, entity_type: EntityType):\n",
    "    for ent in entity_word:\n",
    "        create_new_entity(text, ent, entity_type)\n",
    "    return mask_entity(text)\n",
    "\n",
    "mask_entity_custom(text, [\"bro\"], EntityType.Law)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger\n",
    "\n",
    "POS tagger yang dibawah dibuat berdasarkan dokumentasi `SpaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Hello',\n",
       "  'lemma': 'hello',\n",
       "  'pos': 'INTJ',\n",
       "  'tag': 'UH',\n",
       "  'dep': 'intj',\n",
       "  'shape': 'Xxxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'bro',\n",
       "  'lemma': 'bro',\n",
       "  'pos': 'NOUN',\n",
       "  'tag': 'NN',\n",
       "  'dep': 'npadvmod',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': ',',\n",
       "  'lemma': ',',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': ',',\n",
       "  'dep': 'punct',\n",
       "  'shape': ',',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'i',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'x',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'live',\n",
       "  'lemma': 'live',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VBP',\n",
       "  'dep': 'ROOT',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'in',\n",
       "  'lemma': 'in',\n",
       "  'pos': 'ADP',\n",
       "  'tag': 'IN',\n",
       "  'dep': 'prep',\n",
       "  'shape': 'xx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'Indonesia',\n",
       "  'lemma': 'Indonesia',\n",
       "  'pos': 'PROPN',\n",
       "  'tag': 'NNP',\n",
       "  'dep': 'pobj',\n",
       "  'shape': 'Xxxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'and',\n",
       "  'lemma': 'and',\n",
       "  'pos': 'CCONJ',\n",
       "  'tag': 'CC',\n",
       "  'dep': 'cc',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'i',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'x',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'love',\n",
       "  'lemma': 'love',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VBP',\n",
       "  'dep': 'conj',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'lasagna',\n",
       "  'lemma': 'lasagna',\n",
       "  'pos': 'PROPN',\n",
       "  'tag': 'NNP',\n",
       "  'dep': 'dobj',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': '.',\n",
       "  'lemma': '.',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': '.',\n",
       "  'dep': 'punct',\n",
       "  'shape': '.',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'Also',\n",
       "  'lemma': 'also',\n",
       "  'pos': 'ADV',\n",
       "  'tag': 'RB',\n",
       "  'dep': 'advmod',\n",
       "  'shape': 'Xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': ',',\n",
       "  'lemma': ',',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': ',',\n",
       "  'dep': 'punct',\n",
       "  'shape': ',',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'i',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'x',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'love',\n",
       "  'lemma': 'love',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VBP',\n",
       "  'dep': 'ROOT',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'burgers',\n",
       "  'lemma': 'burger',\n",
       "  'pos': 'NOUN',\n",
       "  'tag': 'NNS',\n",
       "  'dep': 'dobj',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'and',\n",
       "  'lemma': 'and',\n",
       "  'pos': 'CCONJ',\n",
       "  'tag': 'CC',\n",
       "  'dep': 'cc',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'pizzas',\n",
       "  'lemma': 'pizza',\n",
       "  'pos': 'NOUN',\n",
       "  'tag': 'NNS',\n",
       "  'dep': 'conj',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': '.',\n",
       "  'lemma': '.',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': '.',\n",
       "  'dep': 'punct',\n",
       "  'shape': '.',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'Man',\n",
       "  'lemma': 'Man',\n",
       "  'pos': 'PROPN',\n",
       "  'tag': 'NNP',\n",
       "  'dep': 'intj',\n",
       "  'shape': 'Xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': ',',\n",
       "  'lemma': ',',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': ',',\n",
       "  'dep': 'punct',\n",
       "  'shape': ',',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'i',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'x',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'wish',\n",
       "  'lemma': 'wish',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VBP',\n",
       "  'dep': 'ROOT',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'i',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'x',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'could',\n",
       "  'lemma': 'could',\n",
       "  'pos': 'AUX',\n",
       "  'tag': 'MD',\n",
       "  'dep': 'aux',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'get',\n",
       "  'lemma': 'get',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VB',\n",
       "  'dep': 'ccomp',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'some',\n",
       "  'lemma': 'some',\n",
       "  'pos': 'DET',\n",
       "  'tag': 'DT',\n",
       "  'dep': 'det',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'pizza',\n",
       "  'lemma': 'pizza',\n",
       "  'pos': 'NOUN',\n",
       "  'tag': 'NN',\n",
       "  'dep': 'dobj',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'right',\n",
       "  'lemma': 'right',\n",
       "  'pos': 'ADV',\n",
       "  'tag': 'RB',\n",
       "  'dep': 'advmod',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'now',\n",
       "  'lemma': 'now',\n",
       "  'pos': 'ADV',\n",
       "  'tag': 'RB',\n",
       "  'dep': 'advmod',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': '.',\n",
       "  'lemma': '.',\n",
       "  'pos': 'PUNCT',\n",
       "  'tag': '.',\n",
       "  'dep': 'punct',\n",
       "  'shape': '.',\n",
       "  'is_alpha': False,\n",
       "  'is_stop': False},\n",
       " {'text': 'I',\n",
       "  'lemma': 'I',\n",
       "  'pos': 'PRON',\n",
       "  'tag': 'PRP',\n",
       "  'dep': 'nsubj',\n",
       "  'shape': 'X',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'worked',\n",
       "  'lemma': 'work',\n",
       "  'pos': 'VERB',\n",
       "  'tag': 'VBD',\n",
       "  'dep': 'ROOT',\n",
       "  'shape': 'xxxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False},\n",
       " {'text': 'at',\n",
       "  'lemma': 'at',\n",
       "  'pos': 'ADP',\n",
       "  'tag': 'IN',\n",
       "  'dep': 'prep',\n",
       "  'shape': 'xx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': True},\n",
       " {'text': 'itb',\n",
       "  'lemma': 'itb',\n",
       "  'pos': 'NOUN',\n",
       "  'tag': 'NN',\n",
       "  'dep': 'pobj',\n",
       "  'shape': 'xxx',\n",
       "  'is_alpha': True,\n",
       "  'is_stop': False}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_tagger(text):\n",
    "    data = []\n",
    "    for token in text:\n",
    "        token_data = {\n",
    "            'text' : token.text,\n",
    "            'lemma' : token.lemma_,\n",
    "            'pos' : token.pos_,\n",
    "            'tag' : token.tag_,\n",
    "            'dep' : token.dep_,\n",
    "            'shape' : token.shape_,\n",
    "            'is_alpha' : token.is_alpha,\n",
    "            'is_stop' : token.is_stop\n",
    "        }\n",
    "        data.append(token_data)\n",
    "    return data\n",
    "pos_tagger(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Chunking\n",
    "\n",
    "Pada `SpaCy`, hanya terdapat Noun Chunks, sedangkan Verb Phrase Chunks tidak disediakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[i, Indonesia, i, lasagna, i, burgers, pizzas, i, i, some pizza, I, itb]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noun_phrase_chunk(text):\n",
    "    return [chunk for chunk in text.noun_chunks]\n",
    "noun_phrase_chunk(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
